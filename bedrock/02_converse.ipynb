{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9ac3a1-6db7-467c-8ac2-9deb3b1653c2",
   "metadata": {},
   "source": [
    "# Referências da Documentação AWS:\n",
    "\n",
    "- https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference-examples.html\n",
    "\n",
    "- https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse.html\n",
    "\n",
    "- https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/bedrock-runtime/client/converse_stream.html\n",
    "\n",
    "# Dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5494b27c-5afa-4f21-b631-73af622ab9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "from datetime import datetime, timezone"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5d20fa-6fe2-4735-bab2-8d8ec6e29a22",
   "metadata": {},
   "source": [
    "# Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d149f9d1-a20e-47b1-8ef9-c15c14162ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pasta local para salvar respostas das chamadas AWS\n",
    "OUTPUT_FOLDER = \"respostas\"\n",
    "\n",
    "# ID do modelo Amazon Nova Lite (modelo de linguagem da AWS)\n",
    "MODEL_ID = 'amazon.nova-lite-v1:0'\n",
    "\n",
    "# ARN do prompt criado no AWS Prompt Manager\n",
    "PROMPT_ARN = 'arn:aws:bedrock:us-east-1:123456789012:prompt/AABBCC' # Se quiser usar a versão `DRAFT` é só deixar sem id, se quise especificar uma id, basta coloca :id (ex. `:1`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89f2549-253e-45b1-b653-8cc843d55cf6",
   "metadata": {},
   "source": [
    "# Configuração de diretórios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5000cbc6-afd6-411a-9f6c-e55f02ece905",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_FOLDER, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67a532b-e64d-4522-95bd-e30766fa148b",
   "metadata": {},
   "source": [
    "# Configurações do AWS BEDROCK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e73cb6-ec25-4376-8598-5a1d46164bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializa o cliente do Bedrock Runtime para fazer chamadas à API\n",
    "bedrock_runtime_client = boto3.client('bedrock-runtime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdb4208-e6ad-4858-926a-6756d8d149da",
   "metadata": {},
   "source": [
    "# Configuração da requisição"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35dc4b9-319e-4b6e-adc8-0a100a355773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histórico da conversa\n",
    "conversation_history = []\n",
    "\n",
    "# Primeira pergunta\n",
    "conversation_history.append({\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"Qual é a capital do Brasil?\"}]\n",
    "})\n",
    "\n",
    "# Configurações\n",
    "inference_config = {\"temperature\": 0.5, \"maxTokens\": 400}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a53d4fb-14db-4806-af2e-eeaecd5162e2",
   "metadata": {},
   "source": [
    "# Teste do método converse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b40fc0-9c7a-4863-a125-df85d99d2a31",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Primeira chamada\n",
    "response = bedrock_runtime_client.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=conversation_history,\n",
    "        inferenceConfig=inference_config)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb686f76-e828-4173-bd8e-c4970e1cad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar os resultados em um arquivo JSON\n",
    "with open(f\"{OUTPUT_FOLDER}/converse_01.json\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(response, json_file, ensure_ascii=False, indent=4, default=str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206bcef5-f74e-45fe-9863-cf3356b53f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar resposta do modelo ao contexto\n",
    "model_response = response['output']['message']['content'][0]['text']\n",
    "print(model_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787b1318-aebf-4e7f-880e-aca42d14d7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veja estado atual da conversa\n",
    "print(json.dumps(conversation_history, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a4f251-1ab1-445d-9019-1799e4ebbaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar a resposta ao histórico de conversa\n",
    "conversation_history.append({\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": [{\"text\": model_response}]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49beda4-fe2f-4904-835c-8aec8309d2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veja estado atual da conversa\n",
    "print(json.dumps(conversation_history, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd17f2b-a649-41c4-b4e6-138a5edd32c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar uma nova pergunta antes da segunda chamada\n",
    "conversation_history.append({\n",
    "    \"role\": \"user\", \n",
    "    \"content\": [{\"text\": \"Me conte mais sobre essa cidade\"}]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad05091-21e7-4f7b-92ac-6bc1788e7403",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confira o histórico\n",
    "print(json.dumps(conversation_history, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7288240-50f1-4ff1-b269-2935bb8c1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segunda pergunta relacionada\n",
    "response = bedrock_runtime_client.converse(\n",
    "        modelId=MODEL_ID,\n",
    "        messages=conversation_history,\n",
    "        inferenceConfig=inference_config)    \n",
    "\n",
    "# Salvar os resultados em um arquivo JSON\n",
    "with open(f\"{OUTPUT_FOLDER}/converse_02.json\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(response, json_file, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "# Adicionar resposta do modelo ao contexto\n",
    "model_response = response['output']['message']['content'][0]['text']\n",
    "print(model_response)\n",
    "\n",
    "# Confira uso de tokens\n",
    "print(f\"Uso de tokens: {response['usage']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b2c3d4-e5f6-7890-1234-567890abcdef",
   "metadata": {},
   "source": [
    "# Exemplo com Prompt Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c3d4e5-f6g7-8901-2345-678901bcdefg",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variáveis para o prompt de agendamento da barbearia\n",
    "prompt_variables = {\n",
    "    'usuario_nome': {'text': 'Ana'},\n",
    "    'filme_sinopse': {'text': 'Uma jovem princesa embarca em uma aventura mágica para salvar seu reino, descobrindo poderes especiais e fazendo novos amigos pelo caminho.'},    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d4e5f6-g7h8-9012-3456-789012cdefgh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chamada usando Prompt Manager\n",
    "# IMPORTANTE: Quando usamos um prompt do Prompt Manager, as configurações\n",
    "# de inferência (temperature, maxTokens, etc.) foram definidas no próprio\n",
    "# prompt através do console AWS, não podem ser sobrescritas em runtime.\n",
    "# \n",
    "# ❌ ERRO: Usar inferenceConfig aqui resultará em ValidationException:\n",
    "# \"Conflict encountered. Overriding 'inferenceConfig' during runtime is not yet supported\"\n",
    "# \n",
    "# ✅ CORRETO: Omitir inferenceConfig e usar apenas promptVariables\n",
    "response_prompt = bedrock_runtime_client.converse(\n",
    "    modelId=PROMPT_ARN,\n",
    "    promptVariables=prompt_variables\n",
    ")\n",
    "\n",
    "# Exibir resposta\n",
    "message_text = response_prompt['output']['message']['content'][0]['text']\n",
    "print(\"Mensagem gerada pelo Prompt Manager:\")\n",
    "print(message_text)\n",
    "\n",
    "# Salvar resultado\n",
    "with open(f\"{OUTPUT_FOLDER}/converse_03.json\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(response_prompt, json_file, ensure_ascii=False, indent=4, default=str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23c8ac5-99f6-4082-a4d6-b0e8958333f9",
   "metadata": {},
   "source": [
    "# Teste do método converse_stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60b2aa5-96a4-4bbe-8c18-c615192d98c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste do método converse_stream\n",
    "conversation_stream = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [{\"text\": \"Resuma a história do Brasil em 4 parágrafos.\"}]\n",
    "    }\n",
    "]\n",
    "\n",
    "response_stream = bedrock_runtime_client.converse_stream(\n",
    "    modelId=MODEL_ID,\n",
    "    messages=conversation_stream,\n",
    "    inferenceConfig=inference_config\n",
    ")\n",
    "\n",
    "# Salvar os resultados em um arquivo JSON\n",
    "with open(f\"{OUTPUT_FOLDER}/converse_04.json\", 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(response_stream, json_file, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "# Variável para acumular o texto completo da resposta streaming\n",
    "texto_stream = \"\"\n",
    "\n",
    "print(\"\\nResposta em tempo real: \", end=\"\", flush=True)\n",
    "\n",
    "# Processa cada evento do stream de resposta\n",
    "for event in response_stream['stream']:\n",
    "    # Verifica se o evento contém texto da resposta\n",
    "    if 'contentBlockDelta' in event:\n",
    "        # Extrai o texto parcial deste chunk\n",
    "        delta_text = event['contentBlockDelta']['delta']['text']\n",
    "        # Exibe o texto imediatamente (efeito de digitação em tempo real)\n",
    "        print(delta_text, end=\"\", flush=True)\n",
    "        # Acumula o texto para ter a resposta completa no final\n",
    "        texto_stream += delta_text\n",
    "    \n",
    "    # Gera timestamp único para cada chunk (para debug/análise)\n",
    "    timestamp_parcial = datetime.now(timezone.utc).strftime('%Y%m%d%H%M%S%f')\n",
    "    \n",
    "    # Salva cada evento individual para análise posterior\n",
    "    filename_parcial = os.path.join(OUTPUT_FOLDER, f\"partial_{timestamp_parcial}.json\")\n",
    "    with open(filename_parcial, 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(event, json_file, ensure_ascii=False, indent=4, default=str)\n",
    "\n",
    "print(\"\\n\\nTexto completo da resposta:\")\n",
    "print(texto_stream)     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
